{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03cbe0b2-b127-4d89-88dd-14f1abebf455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from byol_pytorch import BYOL\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import importlib\n",
    "import Features.backbone.Custom as Custom\n",
    "from IPython.display import display,clear_output\n",
    "import torchvision as tv\n",
    "import pickle\n",
    "import kornia.augmentation as K\n",
    "import Features.backbone.MiraBest as mb\n",
    "importlib.reload(Custom)\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "# Define the directory of your dataset\n",
    "meerkatn_dir = \"/idia/projects/hippo/Koketso/meerkat\"\n",
    "meerkat_dir = \"/idia/projects/hippo/Koketso/meerkat_resized\"\n",
    "meerkat_curated = \"/idia/projects/hippo/Koketso/meerkat/meerkat_curated\"\n",
    "\n",
    "\n",
    "\n",
    "#Galaxy_zoo directories\n",
    "galaxyzooq_dir = \"/idia/projects/hippo/Koketso/galaxyzoo/resized/galaxy_zoo_class\"\n",
    "galaxyzooqn_dir = \"/idia/projects/hippo/Koketso/galaxyzoo/nonresized/galaxy_zoo_class\"\n",
    "galaxyzooqns_dir = \"/idia/projects/hippo/Koketso/galaxyzoo/nonresized/galaxy_zoo_class_sigma\"\n",
    "\n",
    "\n",
    "\n",
    "galaxyzoo_dir = \"/idia/projects/hippo/Koketso/galaxyzoo/galaxy_zoo\"\n",
    "\n",
    "#Hand_sign dataset\n",
    "\n",
    "hand_dir = \"/idia/projects/hippo/Koketso/Train_Alphabet\"\n",
    "handt_dir = \"/idia/projects/hippo/Koketso/Test_Alphabet\"\n",
    "\n",
    "#Imagenet\n",
    "im13_dir = \"/idia/projects/hippo/Koketso/ILSVRC/Data/DET/train/ILSVRC2013_train\"\n",
    "im12_dir =  \"/idia/projects/hippo/Koketso/ILSVRC/Data/DET/test\"\n",
    "dogbreed_dir = \"/idia/projects/hippo/Koketso/dog_breeds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3b38842-60df-451f-ae6a-1ad899c90402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Zoobot\n",
    "\"\"\"\n",
    "\n",
    "model = tv.models.efficientnet_b0(weights = \"IMAGENET1K_V1\")\n",
    "#zoobot_dict = torch.load(\"Features/models_/old_models/checkpoints/epoch=17-step=2808.ckpt\")['state_dict']\n",
    "#loaded_dict = {k: zoobot_dict[\"model.0.features.\" +k] for k in model.features.state_dict()}\n",
    "\n",
    "#model.classifier[1] = torch.nn.Linear(1280,2)\n",
    "#model.load_state_dict(torch.load(\"Features/models_/mirabest_efficientNet_sup.pt\",map_location = torch.device(\"cpu\"))['model_state_dict'])\n",
    "model.classifier[1] = torch.nn.Linear(1280,100)\n",
    "model.classifier[1].weight.data.normal_(0,0.01)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#Resnet\n",
    "\n",
    "model = tv.models.resnet18(weights = \"IMAGENET1K_V1\")\n",
    "\n",
    "#model = torchvision.models.resnet18(weights = \"IMAGENET1K_V1\")\n",
    "\n",
    "model.fc = torch.nn.Linear(512,100)\n",
    "model.fc.weight.data.normal_(0,0.01)\n",
    "model.load_state_dict(torch.load(\"Features/models_/res18byol_0.pt\")['model_state_dict'])\n",
    "\n",
    "\n",
    "#model.load_state_dict(torch.load(\"Featuremodels_/model_handhfzb_resnet18.pt\",map_location = torch.device(\"cpu\")))\n",
    "#model.load_state_dict(torch.load(\"Features/models_/r34trigzbyol.pt\",map_location = torch.device(\"cpu\"))['model_state_dict'])\n",
    "\n",
    "#model.load_state_dict(torch.load(\"Features/models/model_gz_99HVcut.pt\",map_location = torch.device(\"cpu\")))\n",
    "\n",
    "model.train()\n",
    "\n",
    "#model.load_state_dict(torch.load(\"Features/models/modelbyogzresized3.pt\"))\n",
    "\n",
    "#model.load_state_dict(torch.load(\"Features/models/gzcbi_rot_resized_byolstzooms.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7122809-f2af-4832-b42d-65550540927b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\"Features/models_/res18byol_0.pt\")['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae610e96-6938-492e-98cf-d283ae1ea3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet.load_state_dict(torch.load(\"./Features/models/gzcbirotate_improved-net.pt\"))\n",
    "\n",
    "# Here I write an augmentation that includes all rotation angles. and cuts out the edges\n",
    "class RandomRotationWithCrop(K.RandomRotation):\n",
    "    def __init__(self, degrees, crop_size, output_dim = 244,p = 0.5):\n",
    "        super(RandomRotationWithCrop, self).__init__(degrees, p = 1)\n",
    "        #super(RandomRotationWithCrop, self).__init__(crop_size)\n",
    "        self.rotation_transform = K.RandomRotation(degrees)\n",
    "        self.crop_transform = K.CenterCrop(crop_size, keepdim = False,align_corners = True)\n",
    "        self.resize_transform = K.Resize(output_dim)\n",
    "    def __call__(self, x):\n",
    "        if random.random() <self.p:\n",
    "            # Apply random rotation\n",
    "            x = self.rotation_transform(x)\n",
    "\n",
    "            # Apply center crop\n",
    "            x = self.crop_transform(x)\n",
    "            x = self.resize_transform(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "augment_fn = torch.nn.Sequential(\n",
    "    \n",
    "   \n",
    "        #rotation by an angle of 90\n",
    "        K.RandomRotation([90,90], p=0.15),\n",
    "        K.RandomRotation([180,180], p=0.15),\n",
    "        K.RandomRotation([270,270], p=0.15),\n",
    "        RandomRotationWithCrop(degrees = [0,360],crop_size =210,p =0.3),\n",
    "    \n",
    "        kornia.augmentation.RandomResizedCrop([200,200], p = 0.2),\n",
    "        kornia.augmentation.RandomGaussianBlur(kernel_size = [3,3],sigma = [1,2], p =0.1)\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "learner = BYOL(\n",
    "    model,\n",
    "    image_size = 244,\n",
    "    hidden_layer = 'avgpool',\n",
    "    augment_fn = augment_fn\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d5717f7-5559-425b-a72b-ef9b00cf292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "learner = learner.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1c682b9-2532-4d97-9f2e-b6c28ed1ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "resize = 250\n",
    "\n",
    "dir_ = galaxyzoo_dir\n",
    "#dir_ = meerkat_curated\n",
    "\n",
    "dataset = Custom.dataset(dir_)\n",
    "names = [name[0].split('/')[-1] for name in dataset.imgs]\n",
    "importlib.reload(Custom)\n",
    "\n",
    "datasets = Custom.train_val_dataset(dataset, val_split = 0.05)\n",
    "\n",
    "#Traning\n",
    "\n",
    "transformed_train_dataset = Custom.Custom(datasets['train'],\n",
    "                                    names = names,\n",
    "                                    resize = resize,\n",
    "                                   crop = 224,\n",
    "                                   )\n",
    "\n",
    "transformed_val_dataset = Custom.Custom(datasets['val'],\n",
    "                                    names = names,\n",
    "                                    resize = resize,\n",
    "                                   crop = 224,\n",
    "                                   )\n",
    "\n",
    "#Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f0a99c0-7349-48b9-8c32-8059ef927c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(transformed_train_dataset, \n",
    "                    batch_size, \n",
    "                    shuffle = True,\n",
    "                    num_workers = 16)\n",
    "\n",
    "val_loader = DataLoader(transformed_val_dataset, \n",
    "                    batch_size, \n",
    "                    shuffle = True,\n",
    "                    num_workers = 16)\n",
    "\n",
    "patience = 5\n",
    "\n",
    "best_loss = 5000000\n",
    "\n",
    "opt = torch.optim.Adam(learner.parameters(), lr=3e-5)    #3e-7 for gz   -4 for hand  -5 for meerkat_curated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fabf86db-6e7b-402a-936a-dd576a2d1502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mirabest\n",
    "transform = tv.transforms.Compose([\n",
    "                            #tv.transforms.ToPILImage(),\n",
    "                            #tv.transforms.Resize((424,424)),\n",
    "                            tv.transforms.Resize((300,300)),\n",
    "                            tv.transforms.CenterCrop(224),           # So they are compatible with the dnn models\n",
    "                            tv.transforms.Grayscale(num_output_channels=3),\n",
    "                            tv.transforms.ToTensor(),\n",
    "\n",
    "                            #tv.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                            ])\n",
    "\n",
    "#transformed_dataset = mb.MBFRConfident(root='./batches', train=True, download=True, transform=transform) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f39420e-1526-44aa-ac24-7fa075878fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch epoch :2 Loss :0.17953573167324066\n",
      "Batch epoch :2 Loss :0.22150404751300812\n",
      "Batch epoch :2 Loss :0.35205909609794617\n",
      "Batch epoch :2 Loss :0.2068321406841278\n",
      "Batch epoch :2 Loss :0.054528430104255676\n",
      "Batch epoch :2 Loss :0.24212221801280975\n",
      "Batch epoch :2 Loss :0.07840163260698318\n",
      "Batch epoch :2 Loss :0.08725753426551819\n",
      "Batch epoch :2 Loss :0.278033047914505\n",
      "Batch epoch :2 Loss :0.26980650424957275\n",
      "Batch epoch :2 Loss :0.05299478396773338\n",
      "Batch epoch :2 Loss :0.06412839889526367\n",
      "Batch epoch :2 Loss :0.07730522751808167\n",
      "Batch epoch :2 Loss :0.056511957198381424\n",
      "Batch epoch :2 Loss :0.4167291820049286\n",
      "Batch epoch :2 Loss :0.20618194341659546\n",
      "Batch epoch :2 Loss :0.07312589138746262\n",
      "Batch epoch :2 Loss :0.20156630873680115\n",
      "Batch epoch :2 Loss :0.25471508502960205\n",
      "Batch epoch :2 Loss :0.07100146263837814\n",
      "Batch epoch :2 Loss :0.25665727257728577\n",
      "Batch epoch :2 Loss :0.3061891794204712\n",
      "Batch epoch :2 Loss :0.34192773699760437\n",
      "Batch epoch :2 Loss :0.06397383660078049\n",
      "Batch epoch :2 Loss :0.2429957538843155\n",
      "Batch epoch :2 Loss :0.17235518991947174\n",
      "Batch epoch :2 Loss :0.05589926615357399\n",
      "Batch epoch :2 Loss :0.2680213749408722\n",
      "Batch epoch :2 Loss :0.06842260807752609\n",
      "Batch epoch :2 Loss :0.07058662921190262\n",
      "Batch epoch :2 Loss :0.09731514006853104\n",
      "Batch epoch :2 Loss :0.2286481112241745\n",
      "Batch epoch :2 Loss :0.08215732127428055\n",
      "Batch epoch :2 Loss :0.284579336643219\n",
      "Batch epoch :2 Loss :0.2622017562389374\n",
      "Batch epoch :2 Loss :0.24786458909511566\n",
      "Batch epoch :2 Loss :0.10436635464429855\n",
      "Batch epoch :2 Loss :0.22086015343666077\n",
      "Batch epoch :2 Loss :0.1121734008193016\n",
      "Batch epoch :2 Loss :0.10450156778097153\n",
      "Batch epoch :2 Loss :0.2617868483066559\n",
      "Batch epoch :2 Loss :0.06403183192014694\n",
      "Batch epoch :2 Loss :0.10609844326972961\n",
      "Batch epoch :2 Loss :0.08394653350114822\n",
      "Batch epoch :2 Loss :0.0712105855345726\n",
      "Batch epoch :2 Loss :0.20313125848770142\n",
      "Batch epoch :2 Loss :0.26346081495285034\n",
      "Batch epoch :2 Loss :0.3205321729183197\n",
      "Batch epoch :2 Loss :0.050983261317014694\n",
      "Batch epoch :2 Loss :0.2557068467140198\n",
      "Batch epoch :2 Loss :0.07128895819187164\n",
      "Batch epoch :2 Loss :0.3500829339027405\n",
      "Batch epoch :2 Loss :0.23022669553756714\n",
      "Batch epoch :2 Loss :0.06086740270256996\n",
      "Batch epoch :2 Loss :0.39790546894073486\n",
      "Batch epoch :2 Loss :0.2674884796142578\n",
      "Batch epoch :2 Loss :0.07098798453807831\n",
      "Batch epoch :2 Loss :0.22509746253490448\n",
      "Batch epoch :2 Loss :0.22252418100833893\n",
      "Batch epoch :2 Loss :0.0544465035200119\n",
      "Batch epoch :2 Loss :0.21078072488307953\n",
      "Batch epoch :2 Loss :0.27518796920776367\n",
      "Batch epoch :2 Loss :0.05895106494426727\n",
      "Batch epoch :2 Loss :0.10898244380950928\n",
      "Batch epoch :2 Loss :0.19315379858016968\n",
      "Batch epoch :2 Loss :0.30391114950180054\n",
      "Batch epoch :2 Loss :0.18853381276130676\n",
      "Batch epoch :2 Loss :0.055656593292951584\n",
      "Batch epoch :2 Loss :0.06440908461809158\n",
      "Batch epoch :2 Loss :0.08120182156562805\n",
      "Batch epoch :2 Loss :0.07082561403512955\n"
     ]
    }
   ],
   "source": [
    "loss_saved = [[],[]]\n",
    "loss_pickle = \"./Features/losses/res18byol_1.plk\"\n",
    "\n",
    "\n",
    "with open(loss_pickle,'wb') as file:\n",
    "    pickle.dump(loss_saved,file)\n",
    "for epoch in range(300):\n",
    "    loss_ = 0.0\n",
    "    for i,Images in enumerate(loader):\n",
    "        Images = Images[0]\n",
    "        #send imaged to device\n",
    "        images = Images.to(device)\n",
    "        #optain loss\n",
    "        loss = learner(images)\n",
    "        \n",
    "        #optimization steps\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        learner.update_moving_average() #update moving average of target encoder\n",
    "        loss_ += loss.item()\n",
    "        #display(progress)\n",
    "        if i%5 ==0:\n",
    "            print(\"Batch epoch :\"+ str(epoch) + \" Loss :\" + str(loss.item()))\n",
    "        try:\n",
    "            if i%500 ==0:\n",
    "                clear_output(wait = True)\n",
    "   \n",
    "        except:\n",
    "            print(\"didnt work\")\n",
    "    loss_saved[0].append(loss_)\n",
    "    \n",
    "    \n",
    "    #Implementing the early stopping\n",
    "    # Validate\n",
    "    clear_output(wait = True)\n",
    "    print(\"Validating\")\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for i, val_images in enumerate(val_loader):\n",
    "            \n",
    "            val_images = val_images[0]\n",
    "            val_images = val_images.to(device)\n",
    "            v_loss = learner(val_images)\n",
    "            val_loss += v_loss.item() \n",
    "            \n",
    "            loss_saved[1].append(val_loss)\n",
    "        print(\"Validation loss: \",val_loss)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        \n",
    "        best_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, \"./Features/models_/res18byol_1.pt\")\n",
    "        loss_saved[1].append(loss_)\n",
    "    \n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    # Check if early stopping criteria are met\n",
    "    if counter >= patience:\n",
    "        print(\"Early stopping: No improvement in validation loss for {} epochs\".format(patience))\n",
    "        break\n",
    "        \n",
    "    #save model and  loss\n",
    "    #torch.save(model.state_dict(), \"./Features/models_/model_gzrcr_res34.pt\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a72681b6-b385-4365-838e-1e80f83b2e56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    val_loss = 0\n",
    "    for i, val_images in enumerate(val_loader):\n",
    "        val_images = val_images[0]\n",
    "        val_images = val_images.to(device)\n",
    "        v_loss = learner(val_images)\n",
    "        val_loss += v_loss.item() \n",
    "            \n",
    "        loss_saved[1].append(val_loss)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc0e077-c93b-4dbe-ad66-efa0a0efd8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('ems.pkl','wb') as file:\n",
    "    pickle.dump(embeddings,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb2083e-46a3-4419-b9b6-f487608a273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./Features/models/gzcbi_rot_resized_byol.pt\")        \n",
    "\n",
    "# save your improved network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepclustering310",
   "language": "python",
   "name": "deepclustering310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
